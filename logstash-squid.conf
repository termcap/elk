## This is taken from https://www.elastic.co/blog/a-practical-introduction-to-logstash refer again for more details
## All this config goes to /etc/logstash/squid.conf
## logstash run as /usr/share/logstash/bin/logstash -r -f /etc/logstash/squid.conf

input {
 file {
   path => ["/tmp/squid.log"]
   ## set sincedb to /dev/null only for testing setups, this will cause logstash to forget everything across runs, reindex.
   sincedb_path => "/dev/null" 
   start_position => "beginning"
  }
}
filter {
 ## use dissect or grok depending upon what needs to be done, here the spaces, "/",":" between the labels act as the seperator
 ## -> matches one or more spaces
 dissect {
   mapping => {
     "message" => "%{timestamp->} %{duration} %{client_address} %{cache_result}/%{status_code} %{bytes} %{request_method} %{url} %{user} %{hierarchy_code}/%{server} %{content_type}"
    }
   remove_field => ["message"]
   ## set the right data type, elasticsearch will respect this
   convert_datatype => {
     "bytes" => "int"
     "duration" => "int"
     "status_code" => "int"
     "timestamp" => "float"
    }
  }
## This will take the timestamp, which is in unix epoch, convert it and make it the timestamp of the event
 date {
   match => [ "timestamp", "UNIX" ]
 }
}

## Send data to elasticsearch and stdout[disable stdout when not testing], create the index mappings and patterns first.
output {
 elasticsearch {
   hosts => ["localhost:9200"]
   index => "proxy-%{+YYYY.MM.dd}"
 }
 
  stdout {
    codec => rubydebug
  }
}
